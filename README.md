# Awesome-LLM-Evaluation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
A curated list of papers, tools, platforms for evaluating LLMs

# Leaderboards
- [Chatbot Arena](https://lmarena.ai) - A crowdsourced LLM benchmarking platform. Users can ask any prompt and rank between models anonymously. Leaderboard standings are computed based on user ratings.

# Papers

# Benchmarks

# Tools
- [DeepEval](https://github.com/confident-ai/deepeval) - Open-source LLM evaluation framework. Similar to Pytest but specialized for unit testing LLM outputs.

# Platforms
- [RELAI](https://relai.ai) - RELAI offers automated benchmark generating tools called *Data Agents* that can transform raw data into samples that can be used for LLM evaluation.

# Other
- [Evaluation Guidebook from HuggingFace](https://github.com/huggingface/evaluation-guidebook) - An excellent resource with nifty tips and tricks for evaluating LLMs.
